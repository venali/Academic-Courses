6: Random forests
=================

Random forests started a revolution in machine learning 20 years ago. For the first time, there was a fast and reliable algorithm which made almost no assumptions about the form of the data, and required almost no preprocessing. In today’s lesson, you’ll learn how a random forest really works, and how to build one from scratch. And, just as importantly, you’ll learn how to interpret random forests to better understand your data.

Video
-----

[![Watch the video](https://img.youtube.com/vi/AdhG64NF76E/maxresdefault.jpg)](https://youtu.be/AdhG64NF76E)

This lesson is based partly on [chapter 9](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) of the [book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).

Lesson notebooks
----------------

*   [How random forests really work](https://www.kaggle.com/code/jhoward/how-random-forests-really-work/)
    
*   [Road to the top, part 1](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1)
    

Links from the lesson
---------------------

*   [How to explain Gradient Boosting](https://explained.ai/gradient-boosting/)
    
*   [“Statistical Modeling: The Two Cultures”](https://www.semanticscholar.org/paper/Statistical-modeling:-The-two-cultures-Breiman/e5df6bc6da5653ad98e754b08f63326c2e52b372) by Leo Breiman
