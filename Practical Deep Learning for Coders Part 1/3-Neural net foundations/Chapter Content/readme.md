3: Neural net foundations
=========================

Today we’ll be learning about the mathematical foundations of deep learning: _Stochastic gradient descent_ (SGD), and the flexibility of linear functions layered with non-linear activation functions. We’ll be focussing particularly on a popular combination called the _Rectified linear function_ (ReLU).

Video
-----
[![Watch the video](https://img.youtube.com/vi/hBBOjCiFcuo/maxresdefault.jpg)](https://youtu.be/hBBOjCiFcuo)



This lesson is based partly on [chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb) of the [book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).

Resources
---------

*   Notebooks for this lesson:
    
    *   [HuggingFace Spaces Pets repository](https://huggingface.co/spaces/jph00/pets/tree/main)
        
    *   [Which image models are best?](https://www.kaggle.com/code/jhoward/which-image-models-are-best/)
        
    *   [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)
        
*   Other resources for the lesson
    
    *   Titanic spreadsheet: see the [course repository](https://github.com/fastai/course22)
        
    *   Titanic data (training CSV) can be downloaded from [Kaggle](https://www.kaggle.com/competitions/titanic/)
        
*   [Solutions](https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253) to chapter 4 questions from the book
    

Links from the lesson
---------------------

*   [Know your pet](https://gettoknowyourpet.com/)
    
*   [“Lesson 0”](https://www.youtube.com/watch?v=gGxe2mN3kAg)
